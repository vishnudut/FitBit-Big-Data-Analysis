# -*- coding: utf-8 -*-
"""Bigdata-Project-PySpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SSx-tzrZKf7VDJSgsd3axECCX8FKWxjr
"""

import pandas as pd
import os

!pip install pyspark

# Commented out IPython magic to ensure Python compatibility.
!pip install ipython-autotime
# %load_ext autotime

from google.colab import drive
drive.mount('/content/drive')

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
import plotly.express as px
from pyspark.sql.window import Window

scala_version = '2.12'  # TODO: Ensure this is correct
spark_version = '3.2.1'

packages = [
    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',
    'org.apache.kafka:kafka-clients:3.2.0'
]
spark = SparkSession.builder\
        .master("local")\
        .appName("Colab")\
        .config('spark.ui.port', '4050')\
        .config("spark.jars.packages", ",".join(packages))\
        .getOrCreate()

spark

!curl -sSOL https://dlcdn.apache.org/kafka/3.3.1/kafka_2.12-3.3.1.tgz

!tar -xzf kafka_2.12-3.3.1.tgz

!./kafka_2.12-3.3.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.12-3.3.1/config/zookeeper.properties
!./kafka_2.12-3.3.1/bin/kafka-server-start.sh -daemon ./kafka_2.12-3.3.1/config/server.properties
!echo "Waiting for 10 secs until kafka and zookeeper services are up and running"
!sleep 10

def write_kafka(topic_name,file_path):
  df = spark.read.option("header",True).options(inferSchema='True').csv(file_path)
  os.system(f"./kafka_2.12-3.3.1/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic {topic_name}")
  df.select(F.to_json(F.struct("*")).alias("value"))\
  .selectExpr("CAST(value AS STRING)")\
  .write\
  .format("kafka")\
  .option("kafka.bootstrap.servers", "127.0.0.1:9092")\
  .option("topic", topic_name)\
  .save()
  return df.schema

def read_kafka(topic_name,schema):
  df1 = spark \
  .read \
  .format("kafka") \
  .option("kafka.bootstrap.servers", "127.0.0.1:9092") \
  .option("subscribe", topic_name) \
  .load()
  df1 = df1.withColumn("value", F.col("value").cast(StringType()))

  info_dataframe = df1.select(
          F.from_json(F.col("value"), schema).alias("sample"), "timestamp"
      )
  info_dataframe = info_dataframe.select("sample.*")
  return info_dataframe

#Data write and data in kafka

file_path = "/content/drive/MyDrive/fitbitData/dailyActivity_merged.csv"
topic_name = "activity_daily"
schema = write_kafka(topic_name,file_path)
userActivity = read_kafka(topic_name,schema)

userActivity.printSchema()

userActivity.show()

userActivity.groupBy("Id").agg(F.sum("TotalSteps"))

df = userActivity.coalesce(10)

df.groupBy("Id").agg(F.sum("TotalSteps"))

pandasDf = userActivity.toPandas()

fig = px.scatter(pandasDf,x = "TotalSteps",y="Calories",color="TotalDistance")
fig2 = px.scatter(pandasDf,x="TotalDistance",y = "Calories")
fig.show()
fig2.show()

#Analyzing the trend between Total steps and the number of calories


grp_data = userActivity.groupBy('Id').agg(avg('TotalSteps').alias('Steps'),avg('Calories').alias('Calories'))
pd_data = grp_data.toPandas()
px.scatter(pd_data,x ='Steps',y='Calories').show() #SWARNA - UPDATE : WAY TO SHOW THE ID IN THE CHART

#We are going to analyze how the users of fitbit are walking through the tracked days.
walking_data = userActivity.groupBy('ActivityDate').agg(avg('TotalSteps').alias('TotalSteps')).orderBy('ActivityDate')
pd_walking_data = walking_data.toPandas()
pd_walking_data['datetime'] = pd.to_datetime(pd_walking_data.ActivityDate)
pd_walking_data.sort_values('datetime',inplace=True)
walking_fig = px.line(pd_walking_data, x = 'ActivityDate', y= 'TotalSteps')
walking_fig.show()

#Data write and data in kafka

file_path = "/content/drive/MyDrive/fitbitData/heartrate_seconds_merged.csv"
topic_name = "heartrate_seconds"
schema = write_kafka(topic_name,file_path)
heartRate = read_kafka(topic_name,schema)

heartRate.show()

#Extracting Time from the Timestamp column
spark.conf.set("spark.sql.legacy.timeParserPolicy","LEGACY")
heartRate = heartRate.withColumn("heart_time",date_format(to_timestamp(col("Time"),"mm/dd/yyyy hh:mm:ss aa"),"HH:mm:ss"))

#Extracting Date from the TimeStamp column
heartRate = heartRate.withColumn("Date",to_date(heartRate.Time, "mm/dd/yyyy HH:mm:ss"))

heartRate.show()

heartRate = heartRate.withColumn("Hour", hour("heart_time"))

heartRate.show()

#Get the average heart rate per hour
avg_heart_rate = heartRate.groupBy('Id','Date','Hour').agg(avg('Value')).orderBy(['Hour','Id'])
# avg_heart_rate.show()

avg_heart_rate_hour = avg_heart_rate.groupBy('Hour','Id').agg(avg('avg(Value)')).orderBy(['Id','Hour'])
# avg_heart_rate_hour.show()
#Average heart_rate trend in each hour for all people
avg_heart_rate_general = avg_heart_rate_hour.groupBy("Hour").agg(avg('avg(avg(Value))').alias('Heart Rate')).orderBy("Hour")

#Convert to pandas Df
avg_heart_rate_general = avg_heart_rate_general.toPandas()

#Plotting the trend in the heart rate variations
fig = px.line(avg_heart_rate_general,x = 'Hour', y="Heart Rate")
fig.show()

#Calorie Analysis
file_path = "/content/drive/MyDrive/fitbitData/hourlyCalories_merged.csv"
topic_name = "hourly_calories"
schema = write_kafka(topic_name,file_path)
calorie_df = read_kafka(topic_name,schema)

calorie_df = calorie_df.distinct()
calorie_df = calorie_df.na.drop('any')
calorie_df = calorie_df.withColumn('ActivityHour', from_unixtime(unix_timestamp(col(('ActivityHour')), 'MM/dd/yyyy hh:mm:ss a'),'YYYY-MM-dd HH:mm:ss'))
calorie_df.show()

calorie_df.show()

#Create a Separate date and time column in the same dataframe.
calorie_df = calorie_df.withColumn("Date",date_format(col("ActivityHour"),"yyyy-MM-dd"))
calorie_df = calorie_df.withColumn("Hour", hour(col("ActivityHour")))

#Now first group by ID, Hour and Date.
calorie_avg = calorie_df.groupBy('Id','Hour','Date').agg(avg('Calories').alias('Calories')).orderBy(['Id','Hour'])

#Now we will group by the Hour and ID.

calorie_avg_day = calorie_avg.groupBy('Id','Hour').agg(avg('Calories').alias('Calories')).orderBy(['Id','Hour']);

#The avg calories per hour in general.

calorie_avg_general = calorie_avg_day.groupBy('Hour').agg(avg('Calories').alias('Calories')).orderBy('Hour');
calorie_avg_general.show()

def categorize(hour):
  if hour >=0 and hour <6:
    zone = 'Night'
  elif hour>=6 and hour <12:
    zone = 'Morning'
  elif hour>=12 and hour <18:
    zone = 'Afternoon'
  else:
    zone = 'Evening'
  return zone

zone_function = udf(categorize,StringType())
calorie_avg_general = calorie_avg_general.withColumn("Zone", zone_function(calorie_avg_general.Hour))
calorie_avg_general.show()

df_calorie = calorie_avg_general.toPandas()

fig = px.pie(df_calorie,values = 'Calories',names='Zone')
fig2 = px.line(df_calorie,x = 'Hour', y="Calories")
fig.show()
fig2.show()

file_path = "/content/drive/MyDrive/fitbitData/minuteIntensitiesNarrow_merged.csv"
topic_name = "minute_intensities"
schema = write_kafka(topic_name,file_path)
intensity_df = read_kafka(topic_name,schema)

intensity_df = intensity_df.distinct()
intensity_df = intensity_df.na.drop('any')
intensity_df = intensity_df.dropDuplicates(['Id','ActivityMinute'])


#Now we need to convert it to 24 hour format. And then separate Date and hour. Then categorize the intensities and add the different parts of days as zones.
intensity_df = intensity_df.withColumn('ActivityMinute', from_unixtime(unix_timestamp(col(('ActivityMinute')), 'MM/dd/yyyy hh:mm:ss a'),'YYYY-MM-dd HH:mm:ss'))
intensity_df = intensity_df.withColumn("Date",date_format(col("ActivityMinute"),"yyyy-MM-dd"))
intensity_df = intensity_df.withColumn("Hour", hour(col("ActivityMinute")))
intensity_df = intensity_df.withColumn('Intensity_Desc', when(col('Intensity') == '0','Sedentary').when(col('Intensity') == '1','Light').when(col('Intensity') == '2','Moderate').when(col('Intensity') == '3','Very Active'))
intensity_df = intensity_df.withColumn("Zone", zone_function(intensity_df.Hour))
# intensity_df.show()


day_zone_intensity = intensity_df.groupBy('Id','Date','Zone','Intensity_Desc').agg(count('Id').alias('NumberOfMinutes')).orderBy('Id','Date','Zone','Intensity_Desc')

zone_intensity_avg = day_zone_intensity.groupBy('Zone','Intensity_Desc').agg(avg('NumberOfMinutes').alias('Average')).orderBy('Zone','Intensity_Desc')

zone_intensity_sum = day_zone_intensity.groupBy('Zone','Intensity_Desc').agg(sum('NumberOfMinutes').alias('Sum')).orderBy('Zone','Intensity_Desc')

zone_intensity_sum.show()

intensity_df.show()



intensity_df = spark.read.options(inferSchema='True').csv('/content/drive/MyDrive/fitbitData/minuteIntensitiesNarrow_merged.csv', header=True )
intensity_df = intensity_df.distinct()
intensity_df = intensity_df.na.drop('any')
intensity_df = intensity_df.dropDuplicates(['Id','ActivityMinute'])


#Now we need to convert it to 24 hour format. And then separate Date and hour. Then categorize the intensities and add the different parts of days as zones.
intensity_df = intensity_df.withColumn('ActivityMinute', from_unixtime(unix_timestamp(col(('ActivityMinute')), 'MM/dd/yyyy hh:mm:ss a'),'YYYY-MM-dd HH:mm:ss'))
intensity_df = intensity_df.withColumn("Date",date_format(col("ActivityMinute"),"yyyy-MM-dd"))
intensity_df = intensity_df.withColumn("Hour", hour(col("ActivityMinute")))
intensity_df = intensity_df.withColumn('Intensity_Desc', when(col('Intensity') == '0','Sedentary').when(col('Intensity') == '1','Light').when(col('Intensity') == '2','Moderate').when(col('Intensity') == '3','Very Active'))
intensity_df = intensity_df.withColumn("Zone", zone_function(intensity_df.Hour))
# intensity_df.show()


day_zone_intensity = intensity_df.groupBy('Id','Date','Zone','Intensity_Desc').agg(count('Id').alias('NumberOfMinutes')).orderBy('Id','Date','Zone','Intensity_Desc')

zone_intensity_avg = day_zone_intensity.groupBy('Zone','Intensity_Desc').agg(avg('NumberOfMinutes').alias('Average')).orderBy('Zone','Intensity_Desc')

zone_intensity_sum = day_zone_intensity.groupBy('Zone','Intensity_Desc').agg(sum('NumberOfMinutes').alias('Sum')).orderBy('Zone','Intensity_Desc')

zone_intensity_sum.show()

morning_minute_sum = zone_intensity_sum.where(col('Zone') == 'Morning').toPandas()
morning_chart = px.pie(morning_minute_sum,values='Sum',names = 'Intensity_Desc')
morning_chart.show()

afternoon_minute_sum = zone_intensity_sum.where(col('Zone') == 'Afternoon').toPandas()
afternoon_chart = px.pie(afternoon_minute_sum,values='Sum',names = 'Intensity_Desc')
afternoon_chart.show()

evening_minute_sum = zone_intensity_sum.where(col('Zone') == 'Evening').toPandas()
evening_chart = px.pie(evening_minute_sum,values='Sum',names = 'Intensity_Desc')
evening_chart.show()

night_minute_sum = zone_intensity_sum.where(col('Zone') == 'Night').toPandas()
night_chart = px.pie(night_minute_sum,values='Sum',names = 'Intensity_Desc')
night_chart.show()

file_path = "/content/drive/MyDrive/fitbitData/sleepDay_merged.csv"
topic_name = "sleepDay"
schema = write_kafka(topic_name,file_path)
sleep_df = read_kafka(topic_name,schema)
sleep_df = sleep_df.distinct()
sleep_df = sleep_df.na.drop('any')
sleep_df.show()

def get_diff(col1,col2):
  return col1-col2

sleep_df = sleep_df.withColumn('TimeToFallSleep',get_diff(col('TotalTimeInBed'),col('TotalMinutesAsleep')))
sleep_df.show()

#Calculate users average time to fall asleep
avg_TimeToFallSleep = sleep_df.groupBy("Id").agg(avg('TimeToFallSleep'))
avg_TimeToFallSleep.show()

# SWARNA - UPDATE : ADD CHART FOR ABOVE

# CHIRAG
file_path = "/content/drive/MyDrive/fitbitData/dailySteps_merged.csv"
topic_name = "dailyStepsData"
schema = write_kafka(topic_name,file_path)
dailyStepsData = read_kafka(topic_name,schema)
dailyStepsData.createOrReplaceTempView("dailyStepsData")

spark.sql("SELECT * from dailyStepsData").show()

dailyStepsData = dailyStepsData.withColumn("Date",to_date(dailyStepsData.ActivityDay, "MM/dd/yyyy"))

dailyStepsData = dailyStepsData.withColumn("Weekday",dayofweek(dailyStepsData.Date))

dailyStepsData = dailyStepsData.withColumn("weekday_str",date_format(dailyStepsData.Date,"EEEE"))

dailyStepsData.show()

dailyStepsData.write.csv("dailySteps.csv")

dailyStepsData.coalesce(10).write.parquet("dailySteps1")

dailyStepsData = spark.read.csv("dailySteps.csv")

dailyStepsDataParquet = spark.read.parquet("dailySteps1")

dailyStepsData.groupBy(dailyStepsData._c5).agg(avg(dailyStepsData._c2).alias("avgSteps"))

dailyStepsDataParquet.groupBy(dailyStepsDataParquet.weekday_str).agg(avg(dailyStepsDataParquet.StepTotal).alias("avgSteps"))

avg_steps = dailyStepsData.groupBy(dailyStepsData.weekday_str).agg(avg(dailyStepsData.StepTotal).alias("avgSteps"))

avg_steps = avg_steps.select("*",
               F.when(F.col("weekday_str") == "Monday", 1)
               .when(F.col("weekday_str") == "Tuesday", 2)
               .when(F.col("weekday_str") == "Wednesday", 3)
               .when(F.col("weekday_str") == "Thursday", 4)
               .when(F.col("weekday_str") == "Friday", 5)
               .when(F.col("weekday_str") == "Saturday", 6)
               .when(F.col("weekday_str") == "Sunday", 7)
               .otherwise(0)
               .alias("weekday_order")
              )
avg_steps_ordered = avg_steps.orderBy(F.col("weekday_order").asc())
avg_steps_ordered.show()

avg_steps_pandas = avg_steps_ordered.toPandas()
fig = px.line(avg_steps_pandas, x="weekday_str", y="avgSteps", title='Average steps by users per day of the week')
fig.show()

dailyStepsData = dailyStepsData.withColumn("StepTotal",dailyStepsData["StepTotal"].cast(IntegerType()))
grouped_df = dailyStepsData.groupBy("Id", "weekday_str").sum("StepTotal")
pandas_df = grouped_df.toPandas()
fig = px.bar(data_frame=pandas_df, x="weekday_str", y="sum(StepTotal)", color = "Id")
fig.show()

dailyStepsPandas = dailyStepsData.toPandas()
fig = px.scatter(dailyStepsPandas,x="weekday_str",y="StepTotal")
fig.show()

file_path = "/content/drive/MyDrive/fitbitData/sleepDay_merged.csv"
topic_name = "sleepDay"
schema = write_kafka(topic_name,file_path)
sleepDf = read_kafka(topic_name,schema)

sleepDf = sleepDf.withColumn("Date",to_date(sleepDf.SleepDay, "MM/dd/yyyy HH:mm:ss"))

sleepDf = sleepDf.withColumn("Weekday",dayofweek(sleepDf.Date))

sleepDf = sleepDf.withColumn("weekday_str",date_format(sleepDf.Date,"EEEE"))

windowSpec  = Window.partitionBy("Id","weekday_str").orderBy("TotalMinutesAsleep")

sleepDf.groupBy(sleepDf.weekday_str).agg(F.collect_list(F.col("TotalMinutesAsleep")).alias("totalminutes"))\
.withColumn("len",F.size(F.col("totalminutes"))).show()

avg_sleep = sleepDf.groupBy(sleepDf.weekday_str).agg(avg(sleepDf.TotalMinutesAsleep).alias("avgSleep"))
avg_sleep.show()

avg_sleep = avg_sleep.select("*",
               F.when(F.col("weekday_str") == "Monday", 1)
               .when(F.col("weekday_str") == "Tuesday", 2)
               .when(F.col("weekday_str") == "Wednesday", 3)
               .when(F.col("weekday_str") == "Thursday", 4)
               .when(F.col("weekday_str") == "Friday", 5)
               .when(F.col("weekday_str") == "Saturday", 6)
               .when(F.col("weekday_str") == "Sunday", 7)
               .otherwise(0)
               .alias("weekday_order")
              )
avg_sleep_ordered = avg_sleep.orderBy(F.col("weekday_order").asc())
# avg_sleep_ordered.show()

avg_sleep_pandas = avg_sleep_ordered.toPandas()
fig = px.line(avg_sleep_pandas, x="weekday_str", y="avgSleep", title='Average sleep by users per day of the week')
fig.show()

file_path = "/content/drive/MyDrive/fitbitData/minuteIntensitiesNarrow_merged.csv"
topic_name = "minuteIntensitiesNarrow"
schema = write_kafka(topic_name,file_path)
minuteIntensities = read_kafka(topic_name,schema)

minuteIntensities_df = minuteIntensities.withColumn('Intensity_Desc', F.when(col('Intensity') == '0','Sedentary').when(col('Intensity') == '1','Light').when(col('Intensity') == '2','Moderate').when(col('Intensity') == '3','Very Active'))
minuteIntensityDf = minuteIntensities_df.withColumn("Date",to_timestamp(minuteIntensities_df.ActivityMinute, "MM/dd/yyyy HH:mm:ss"))
minuteIntensityDf = minuteIntensityDf.withColumn("date_column",to_date(minuteIntensities_df.ActivityMinute, "MM/dd/yyyy"))

minuteIntensityDf = minuteIntensityDf.withColumn('Date1', F.from_unixtime(F.unix_timestamp(F.col(('ActivityMinute')), 'MM/dd/yyyy hh:mm:ss a'),'YYYY-MM-dd HH:mm:ss'))
minuteIntensityDf = minuteIntensityDf.withColumn("hour_column", F.hour(F.col("Date1")))

groupedByMinuteIntensity = minuteIntensityDf.groupBy(minuteIntensityDf.Id,minuteIntensityDf.date_column,minuteIntensityDf.hour_column,minuteIntensityDf.Intensity_Desc).agg(F.count("Intensity_Desc").alias("count_mins"))

groupedByMinuteIntensity = groupedByMinuteIntensity.groupBy(groupedByMinuteIntensity.hour_column,groupedByMinuteIntensity.Intensity_Desc).agg(F.avg(F.col("count_mins")).alias("hour_mins"))
groupedByMinuteIntensity.show()
totalIntensityMinsGroupedByHour = groupedByMinuteIntensity.groupBy(groupedByMinuteIntensity.hour_column).agg(F.sum(groupedByMinuteIntensity.hour_mins))
totalIntensityMinsGroupedByHour.show()

groupedByMinuteIntensity.count()

totalIntensityMinsGroupedByHour.count()

joinedIntensityTable = groupedByMinuteIntensity.join(totalIntensityMinsGroupedByHour,"hour_column","leftouter")

joinedIntensityTable = groupedByMinuteIntensity.join(F.broadcast(totalIntensityMinsGroupedByHour),"hour_column","leftouter")

pandas_df_intensities = joinedIntensityTable.toPandas()
fig = px.bar(pandas_df_intensities, x="hour_column", y="hour_mins", color="Intensity_Desc")
fig.show()

file_path = "/content/drive/MyDrive/fitbitData/hourlyCalories_merged.csv"
topic_name = "hourlyCalories"
schema = write_kafka(topic_name,file_path)
calorie_df = read_kafka(topic_name,schema)

calorie_df = calorie_df.distinct()
calorie_df = calorie_df.na.drop('any')
calorie_df = calorie_df.withColumn('ActivityHour', from_unixtime(unix_timestamp(col(('ActivityHour')), 'MM/dd/yyyy hh:mm:ss a'),'YYYY-MM-dd HH:mm:ss'))
# calorie_df.show()

calorie_df = calorie_df.withColumn("Date",date_format(col("ActivityHour"),"yyyy-MM-dd"))
calorie_df = calorie_df.withColumn("Hour", hour(col("ActivityHour")))
# calorie_df.show()

#Now first group by ID, Hour and Date.
calorie_avg = calorie_df.groupBy('Id','Hour','Date').agg(avg('Calories').alias('Calories')).orderBy(['Id','Hour'])
# calorie_avg.show()

#Now we will group by the Hour and ID.

calorie_avg_day = calorie_avg.groupBy('Id','Hour').agg(avg('Calories').alias('Calories')).orderBy(['Id','Hour']);
# calorie_avg_day.show()

#The avg calories per hour in general.

calorie_avg_general = calorie_avg_day.groupBy('Hour').agg(avg('Calories').alias('Calories')).orderBy('Hour');
# calorie_avg_general.show()

calorie_avg_general_pandas = calorie_avg_general.toPandas()
fig = px.bar(calorie_avg_general_pandas,x = 'Hour',y='Calories')
fig.show()

userActivity.show()

userActivity = userActivity.withColumn("TotalActiveMinutes",userActivity.LightlyActiveMinutes + userActivity.FairlyActiveMinutes  + userActivity.VeryActiveMinutes)

userActivity.show()

!pip install -U scikit-learn

userActivity.show()

import six
corr_dict = {}
for i in userActivity.columns:
    if not( isinstance(userActivity.select(i).take(1)[0][0], six.string_types)):
        print( "Correlation to TotalActiveMinutes for ", i, userActivity.stat.corr('TotalActiveMinutes',i))
        corr_dict[i] = userActivity.stat.corr('TotalActiveMinutes',i)

for key,val in corr_dict.items():
  if val > 0.5 :
    print(key,val)

activity = userActivity.select("TotalSteps","TotalDistance","LightActiveDistance","LightlyActiveMinutes","TotalActiveMinutes")

activity.show()

from pyspark.ml.feature import VectorAssembler
vectorAssembler = VectorAssembler(inputCols = ["TotalSteps","TotalDistance","LightActiveDistance","LightlyActiveMinutes"], outputCol = 'features')
vhouse_df = vectorAssembler.transform(activity)
vhouse_df = vhouse_df.select(['features', 'TotalActiveMinutes'])
vhouse_df.show(3)

splits = vhouse_df.randomSplit([0.7, 0.3])
train_df = splits[0]
test_df = splits[1]

train_df.count()

test_df.count()

from pyspark.ml.regression import LinearRegression

lr = LinearRegression(featuresCol = 'features', labelCol='TotalActiveMinutes', maxIter=10, regParam=0.3, elasticNetParam=0.8)
lr_model = lr.fit(train_df)
print("Coefficients: " + str(lr_model.coefficients))
print("Intercept: " + str(lr_model.intercept))

trainingSummary = lr_model.summary
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)

from pyspark.ml.evaluation import RegressionEvaluator
lr_predictions = lr_model.transform(test_df)
lr_predictions.select("prediction","TotalActiveMinutes","features").show(5)
lr_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="TotalActiveMinutes",metricName="r2")
print("R Squared (R2) on test data = %g" % lr_evaluator.evaluate(lr_predictions))

test_result = lr_model.evaluate(test_df)
print("Root Mean Squared Error (RMSE) on test data = %g" % test_result.rootMeanSquaredError)

predictions = lr_model.transform(test_df)
predictions.select("prediction","TotalActiveMinutes","features").show()

predictions



